{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sequential_mnist.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "3BOMRnzSa7rk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Simple RNN Modles for Sequential MNIST with Tensorflow\n",
        "\n",
        "Based on the work of [Aymeric Damien](https://github.com/aymericdamien/TensorFlow-Examples/) and [Sungjoon](https://github.com/sjchoi86/tensorflow-101/blob/master/notebooks/rnn_mnist_simple.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "yNP2vmzaa7rm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## RNN Overview\n",
        "\n",
        "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" alt=\"nn\" style=\"width: 600px;\"/>\n",
        "\n",
        "References:\n",
        "- [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf), Sepp Hochreiter & Jurgen Schmidhuber, Neural Computation 9(8): 1735-1780, 1997.\n",
        "\n",
        "## MNIST Dataset Overview\n",
        "\n",
        "This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).\n",
        "\n",
        "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
        "\n",
        "To classify images using a recurrent neural network, we consider every image row as a sequence of pixels. Because MNIST image shape is 28*28px, we will then handle 28 sequences of 28 timesteps for every sample.\n",
        "\n",
        "More info: http://yann.lecun.com/exdb/mnist/"
      ]
    },
    {
      "metadata": {
        "id": "avKi_M3hdW7G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## System Information"
      ]
    },
    {
      "metadata": {
        "id": "469NP3vAdc9U",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "400c2f17-74ba-43d5-ccec-2f2f6692a63a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1522578783034,
          "user_tz": -480,
          "elapsed": 5578,
          "user": {
            "displayName": "CeShine Lee",
            "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAYB4/FhqIeNDBAlM/s50-c-k-no/photo.jpg",
            "userId": "114938319508229761672"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install watermark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting watermark\n",
            "  Downloading watermark-1.6.0-py3-none-any.whl\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from watermark)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->watermark)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->watermark)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/lib/python3/dist-packages (from ipython->watermark)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->watermark)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->watermark)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->watermark)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->watermark)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->watermark)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->watermark)\n",
            "Installing collected packages: watermark\n",
            "Successfully installed watermark-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A-fpqklBefZy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "c04ded85-334d-4e69-b64d-3c977f050e13",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1522578794644,
          "user_tz": -480,
          "elapsed": 9542,
          "user": {
            "displayName": "CeShine Lee",
            "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAYB4/FhqIeNDBAlM/s50-c-k-no/photo.jpg",
            "userId": "114938319508229761672"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import random \n",
        "from datetime import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn\n",
        "import numpy as np\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-xqyUfB-hGzC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "c8eebe6a-61be-4143-8dcc-93cd00c76929",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1522578795868,
          "user_tz": -480,
          "elapsed": 1158,
          "user": {
            "displayName": "CeShine Lee",
            "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAYB4/FhqIeNDBAlM/s50-c-k-no/photo.jpg",
            "userId": "114938319508229761672"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "%load_ext watermark\n",
        "%watermark -v -m -p tensorflow,numpy -g"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPython 3.6.3\n",
            "IPython 5.5.0\n",
            "\n",
            "tensorflow 1.6.0\n",
            "numpy 1.14.2\n",
            "\n",
            "compiler   : GCC 7.2.0\n",
            "system     : Linux\n",
            "release    : 4.4.111+\n",
            "machine    : x86_64\n",
            "processor  : x86_64\n",
            "CPU cores  : 2\n",
            "interpreter: 64bit\n",
            "Git hash   :\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Of-jSp2TddTS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Row-by-row Sequential MNIST\n",
        "\n",
        "How RNN model for row-by-row sequential MNIST works from [Sungjoon's notebook](https://github.com/sjchoi86/tensorflow-101/blob/master/notebooks/rnn_mnist_simple.ipynb):\n",
        "![](https://raw.githubusercontent.com/sjchoi86/Tensorflow-101/582cc1d946f0ecbce078e493b8ccb1d7b40684df/notebooks/images/etc/rnn_mnist_look.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "CiiU7wPsi7Im",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Basic LSTM Model\n",
        "Directly taken from [Aymeric Damien's notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/recurrent_network.ipynb)."
      ]
    },
    {
      "metadata": {
        "id": "fCtnM4cdjRPg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Training Parameters\n",
        "learning_rate = 0.001\n",
        "training_steps = 5000\n",
        "batch_size = 128\n",
        "display_step = 200\n",
        "\n",
        "# Network Parameters\n",
        "num_input = 28 # MNIST data input (img shape: 28*28)\n",
        "timesteps = 28 # timesteps\n",
        "num_hidden = 64 # hidden layer num of features\n",
        "num_classes = 10 # MNIST total classes (0-9 digits)\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
        "Y = tf.placeholder(\"float\", [None, num_classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cr8TEextjVFG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Define weights\n",
        "weights = {\n",
        "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
        "}\n",
        "biases = {\n",
        "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BnMayzeIjZ4a",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def RNN(x, weights, biases):\n",
        "\n",
        "    # Prepare data shape to match `rnn` function requirements\n",
        "    # Current data input shape: (batch_size, timesteps, n_input)\n",
        "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
        "\n",
        "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
        "    x = tf.unstack(x, timesteps, 1)\n",
        "\n",
        "    # Define a lstm cell with tensorflow\n",
        "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
        "\n",
        "    # Get lstm cell output\n",
        "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
        "\n",
        "    # Linear activation, using rnn inner loop last output\n",
        "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xSgImScnjaYe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "e9e3bda9-bb2b-41f3-8254-b08a504f4f87",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1522578803786,
          "user_tz": -480,
          "elapsed": 3484,
          "user": {
            "displayName": "CeShine Lee",
            "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAYB4/FhqIeNDBAlM/s50-c-k-no/photo.jpg",
            "userId": "114938319508229761672"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "logits = RNN(X, weights, biases)\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Evaluate model (with test logits, for dropout to be disabled)\n",
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-e3b51778de6b>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B8Alb6-Ljgsc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "108fb22b-80c8-4368-b861-4fc7f668f882",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1522578899954,
          "user_tz": -480,
          "elapsed": 96134,
          "user": {
            "displayName": "CeShine Lee",
            "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAYB4/FhqIeNDBAlM/s50-c-k-no/photo.jpg",
            "userId": "114938319508229761672"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    for step in range(1, training_steps+1):\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        # Reshape data to get 28 seq of 28 elements\n",
        "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
        "                                                                 Y: batch_y})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Calculate accuracy for 128 mnist test images\n",
        "    test_len = 128\n",
        "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
        "    test_label = mnist.test.labels[:test_len]\n",
        "    print(\"Testing Accuracy:\", \\\n",
        "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1, Minibatch Loss= 2.8235, Training Accuracy= 0.102\n",
            "Step 200, Minibatch Loss= 2.2561, Training Accuracy= 0.156\n",
            "Step 400, Minibatch Loss= 2.1433, Training Accuracy= 0.219\n",
            "Step 600, Minibatch Loss= 2.0626, Training Accuracy= 0.398\n",
            "Step 800, Minibatch Loss= 2.0022, Training Accuracy= 0.344\n",
            "Step 1000, Minibatch Loss= 1.9623, Training Accuracy= 0.359\n",
            "Step 1200, Minibatch Loss= 1.9050, Training Accuracy= 0.398\n",
            "Step 1400, Minibatch Loss= 1.8771, Training Accuracy= 0.344\n",
            "Step 1600, Minibatch Loss= 1.8084, Training Accuracy= 0.375\n",
            "Step 1800, Minibatch Loss= 1.6760, Training Accuracy= 0.477\n",
            "Step 2000, Minibatch Loss= 1.7269, Training Accuracy= 0.453\n",
            "Step 2200, Minibatch Loss= 1.6756, Training Accuracy= 0.516\n",
            "Step 2400, Minibatch Loss= 1.6708, Training Accuracy= 0.398\n",
            "Step 2600, Minibatch Loss= 1.6657, Training Accuracy= 0.469\n",
            "Step 2800, Minibatch Loss= 1.4649, Training Accuracy= 0.523\n",
            "Step 3000, Minibatch Loss= 1.4580, Training Accuracy= 0.617\n",
            "Step 3200, Minibatch Loss= 1.4600, Training Accuracy= 0.516\n",
            "Step 3400, Minibatch Loss= 1.4958, Training Accuracy= 0.508\n",
            "Step 3600, Minibatch Loss= 1.4225, Training Accuracy= 0.547\n",
            "Step 3800, Minibatch Loss= 1.2084, Training Accuracy= 0.609\n",
            "Step 4000, Minibatch Loss= 1.2144, Training Accuracy= 0.625\n",
            "Step 4200, Minibatch Loss= 1.2680, Training Accuracy= 0.633\n",
            "Step 4400, Minibatch Loss= 1.3150, Training Accuracy= 0.586\n",
            "Step 4600, Minibatch Loss= 1.2753, Training Accuracy= 0.578\n",
            "Step 4800, Minibatch Loss= 1.2169, Training Accuracy= 0.633\n",
            "Step 5000, Minibatch Loss= 1.0543, Training Accuracy= 0.688\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.5859375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KHcEqoHijfsm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tidier Version of the LSTM Model\n",
        "\n",
        "1. Use LSTMBlockCell, which should be faster than BasicLSTMCell\n",
        "2. Replace manual weight definitions with tf.layers.Dense\n",
        "3. Replace tf.nn.softmax_cross_entropy_with_logits with tf.nn.softmax_cross_entropy_with_logits_v2\n",
        "4. Group graph definition together\n",
        "5. Replace rnn.dynamic_rnn with rnn.static_rnn. (So no need to unstack the tensor.)\n",
        "6. Add a batch_normalization layer between LSTM and Dense layers.\n",
        "7. Add gradient clipping for RNN gradient\n",
        "8. Add a checkpoint saver\n",
        "9. Evaluate test accuracy every N steps (BAD PRACTICE: use a validation set instead)\n",
        "10. Replace GradientDescentOptimizer with RMSPropOptimizer\n",
        "11. Use tf.set_random_seed to control randomness"
      ]
    },
    {
      "metadata": {
        "id": "MPwqdhkEa7ry",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Training Parameters\n",
        "learning_rate = 0.02\n",
        "training_steps = 5000\n",
        "batch_size = 32\n",
        "display_step = 250\n",
        "\n",
        "# Network Parameters\n",
        "num_input = 28 # MNIST data input (img shape: 28*28)\n",
        "timesteps = 28 # timesteps\n",
        "num_hidden = 64 # hidden layer num of features\n",
        "num_classes = 10 # MNIST total classes (0-9 digits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GyGSqEoqa7r2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def RNN(x):\n",
        "    # Define a lstm cell with tensorflow\n",
        "    lstm_cell = rnn.LSTMBlockCell(\n",
        "        num_hidden, forget_bias=1.0)\n",
        "\n",
        "    # Get lstm cell output\n",
        "    # outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
        "    outputs, states = tf.nn.dynamic_rnn(\n",
        "        cell=lstm_cell, inputs=x, time_major=False, dtype=tf.float32)\n",
        "    \n",
        "    output_layer = tf.layers.Dense(\n",
        "        num_classes, activation=None, \n",
        "        kernel_initializer=tf.orthogonal_initializer()\n",
        "    )\n",
        "    return output_layer(tf.layers.batch_normalization(outputs[:, -1, :]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wpQ_5H6Za7r6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "eeb2ecc1-c133-45bf-f6e9-d61949491124",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1522578903962,
          "user_tz": -480,
          "elapsed": 1660,
          "user": {
            "displayName": "CeShine Lee",
            "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAYB4/FhqIeNDBAlM/s50-c-k-no/photo.jpg",
            "userId": "114938319508229761672"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Need to clear the default graph before moving forward\n",
        "tf.reset_default_graph()\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    tf.set_random_seed(1)\n",
        "    # tf Graph input\n",
        "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
        "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
        "    logits = RNN(X)\n",
        "    prediction = tf.nn.softmax(logits)\n",
        "\n",
        "    # Define loss and optimizer\n",
        "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "        logits=logits, labels=Y))\n",
        "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
        "    gvs = optimizer.compute_gradients(loss_op)\n",
        "    capped_gvs = [\n",
        "        (tf.clip_by_norm(grad, 2.), var) if not var.name.startswith(\"dense\") else (grad, var)\n",
        "        for grad, var in gvs]\n",
        "    for _, var in gvs:\n",
        "        if var.name.startswith(\"dense\"):\n",
        "            print(var.name)    \n",
        "    train_op = optimizer.apply_gradients(capped_gvs)  \n",
        "\n",
        "    # Evaluate model (with test logits, for dropout to be disabled)\n",
        "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "    # Initialize the variables (i.e. assign their default value)\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    print(\"All parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.global_variables()]))\n",
        "    print(\"Trainable parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()]))    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/clip_ops.py:113: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "dense/kernel:0\n",
            "dense/bias:0\n",
            "All parameters: 73886\n",
            "Trainable parameters: 24586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "juyI0OmIa7r-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "9bcd73bc-82e5-4b1a-e0a7-ead32c8758df",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1522579064840,
          "user_tz": -480,
          "elapsed": 160740,
          "user": {
            "displayName": "CeShine Lee",
            "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAYB4/FhqIeNDBAlM/s50-c-k-no/photo.jpg",
            "userId": "114938319508229761672"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "best_val_acc = 0.8\n",
        "with tf.Session(graph=graph, config=config) as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    for step in range(1, training_steps+1):\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        # Reshape data to get 28 seq of 28 elements\n",
        "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
        "                                                                 Y: batch_y})\n",
        "            # Calculate accuracy for 128 mnist test images\n",
        "            test_len = 128\n",
        "            test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
        "            test_label = mnist.test.labels[:test_len]\n",
        "            val_acc = sess.run(accuracy, feed_dict={X: test_data, Y: test_label})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc) + \", Test Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(val_acc))\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                save_path = saver.save(sess, \"/tmp/model.ckpt\", global_step=step)\n",
        "                print(\"Model saved in path: %s\" % save_path)\n",
        "    print(\"Optimization Finished!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1, Minibatch Loss= 2.2981, Training Accuracy= 0.156, Test Accuracy= 0.117\n",
            "Step 250, Minibatch Loss= 0.3176, Training Accuracy= 0.906, Test Accuracy= 0.766\n",
            "Step 500, Minibatch Loss= 0.3501, Training Accuracy= 0.844, Test Accuracy= 0.883\n",
            "Model saved in path: /tmp/model.ckpt-500\n",
            "Step 750, Minibatch Loss= 0.0230, Training Accuracy= 1.000, Test Accuracy= 0.945\n",
            "Model saved in path: /tmp/model.ckpt-750\n",
            "Step 1000, Minibatch Loss= 0.0212, Training Accuracy= 1.000, Test Accuracy= 0.945\n",
            "Step 1250, Minibatch Loss= 0.1140, Training Accuracy= 0.969, Test Accuracy= 0.906\n",
            "Step 1500, Minibatch Loss= 0.0110, Training Accuracy= 1.000, Test Accuracy= 0.945\n",
            "Step 1750, Minibatch Loss= 0.0060, Training Accuracy= 1.000, Test Accuracy= 0.961\n",
            "Model saved in path: /tmp/model.ckpt-1750\n",
            "Step 2000, Minibatch Loss= 0.0418, Training Accuracy= 1.000, Test Accuracy= 0.922\n",
            "Step 2250, Minibatch Loss= 0.0168, Training Accuracy= 1.000, Test Accuracy= 0.969\n",
            "Model saved in path: /tmp/model.ckpt-2250\n",
            "Step 2500, Minibatch Loss= 0.0208, Training Accuracy= 1.000, Test Accuracy= 0.992\n",
            "Model saved in path: /tmp/model.ckpt-2500\n",
            "Step 2750, Minibatch Loss= 0.0038, Training Accuracy= 1.000, Test Accuracy= 0.984\n",
            "Step 3000, Minibatch Loss= 0.0024, Training Accuracy= 1.000, Test Accuracy= 1.000\n",
            "Model saved in path: /tmp/model.ckpt-3000\n",
            "Step 3250, Minibatch Loss= 0.0218, Training Accuracy= 1.000, Test Accuracy= 0.953\n",
            "Step 3500, Minibatch Loss= 0.0145, Training Accuracy= 1.000, Test Accuracy= 0.977\n",
            "Step 3750, Minibatch Loss= 0.0098, Training Accuracy= 1.000, Test Accuracy= 0.953\n",
            "Step 4000, Minibatch Loss= 0.0816, Training Accuracy= 0.969, Test Accuracy= 0.953\n",
            "Step 4250, Minibatch Loss= 0.0138, Training Accuracy= 1.000, Test Accuracy= 0.961\n",
            "Step 4500, Minibatch Loss= 0.0106, Training Accuracy= 1.000, Test Accuracy= 0.953\n",
            "Step 4750, Minibatch Loss= 0.0078, Training Accuracy= 1.000, Test Accuracy= 0.945\n",
            "Step 5000, Minibatch Loss= 0.0064, Training Accuracy= 1.000, Test Accuracy= 0.953\n",
            "Optimization Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U3RLjSpqrs-I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pixel-by-pixel Sequential MNIST\n",
        "\n",
        "View every example as a 784 x 1 sequence."
      ]
    },
    {
      "metadata": {
        "id": "Y3gPee075_4o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### CudnnGRU Model\n",
        "\n",
        "Cudnn's implementation of GRU is much faster, but does not support variational dropout. It also does not support CPU mode.\n",
        "\n",
        "Also some new additions:\n",
        "\n",
        "1. Use tf.summary to save logs for Tensorboard\n",
        "2. Use tf.variable_scope to group variables and operations\n",
        "3. Use AdamOptimizer instead of RMSPropOptimizer (latter has some problem with CudnnGRU)"
      ]
    },
    {
      "metadata": {
        "id": "rG96Bh8c2cs4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c524ee7-6ed0-401f-9094-89fcdcba95ed",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1522579066050,
          "user_tz": -480,
          "elapsed": 1152,
          "user": {
            "displayName": "CeShine Lee",
            "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAYB4/FhqIeNDBAlM/s50-c-k-no/photo.jpg",
            "userId": "114938319508229761672"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Training Parameters\n",
        "learning_rate = 0.002\n",
        "training_steps = 5000\n",
        "batch_size = 32\n",
        "display_step = 250\n",
        "total_batch = int(mnist.train.num_examples / batch_size)\n",
        "print(\"Total number of batches:\", total_batch)\n",
        "\n",
        "# Network Parameters\n",
        "num_input = 1 # MNIST data input (img shape: 28*28)\n",
        "timesteps = 28 * 28 # timesteps\n",
        "num_hidden = 128 # hidden layer num of features\n",
        "num_classes = 10 # MNIST total classes (0-9 digits)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of batches: 1718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H6ZQmqtuuVHQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def RNN(x):    \n",
        "    gru = tf.contrib.cudnn_rnn.CudnnGRU(\n",
        "        1, num_hidden,\n",
        "        kernel_initializer=tf.orthogonal_initializer())\n",
        "    outputs, _ = gru(tf.transpose(x, (1, 0, 2)))    \n",
        "    output_layer = tf.layers.Dense(\n",
        "        num_classes, activation=None, \n",
        "        kernel_initializer=tf.orthogonal_initializer(),\n",
        "        trainable=True\n",
        "    )\n",
        "    # Linear activation, using rnn inner loop last output\n",
        "    return output_layer(tf.layers.batch_normalization(outputs[-1, :, :]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hPNBPWsev0LG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4a565c4a-e228-4aef-f26a-202f56ca0c23",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1522579069292,
          "user_tz": -480,
          "elapsed": 1510,
          "user": {
            "displayName": "CeShine Lee",
            "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAYB4/FhqIeNDBAlM/s50-c-k-no/photo.jpg",
            "userId": "114938319508229761672"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    tf.set_random_seed(10)\n",
        "    # tf Graph input\n",
        "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
        "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
        "    # Define weights\n",
        "    logits = RNN(X)\n",
        "    prediction = tf.nn.softmax(logits)\n",
        "\n",
        "    # Define loss and optimizer\n",
        "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "        logits=logits, labels=Y))\n",
        "    \n",
        "    with tf.variable_scope('optimizer'):\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.1)\n",
        "        gvs = optimizer.compute_gradients(loss_op)\n",
        "        capped_gvs = [\n",
        "            (tf.clip_by_norm(grad, 1.), var) if not var.name.startswith(\"dense\") else (grad, var)\n",
        "            for grad, var in gvs]\n",
        "        for _, var in gvs:\n",
        "            if var.name.startswith(\"dense\"):\n",
        "                print(var.name)\n",
        "        train_op = optimizer.apply_gradients(capped_gvs)  \n",
        "    \n",
        "    with tf.variable_scope('summarize_gradients'):\n",
        "        for grad, var in gvs:\n",
        "            norm = tf.norm(tf.clip_by_norm(grad, 10.), ord=2)\n",
        "            tf.summary.histogram(var.name.replace(\":\", \"_\") + '/gradient_l2', \n",
        "                                 tf.where(tf.is_nan(norm), tf.zeros_like(norm), norm))\n",
        "        for grad, var in capped_gvs:\n",
        "            norm = tf.norm(grad, ord=2)\n",
        "            tf.summary.histogram(var.name.replace(\":\", \"_\") + '/gradient_clipped_l2', \n",
        "                                 tf.where(tf.is_nan(norm), tf.zeros_like(norm), norm))\n",
        "\n",
        "    merged_summary_op = tf.summary.merge_all()\n",
        "    \n",
        "    with tf.variable_scope('evaluate'):\n",
        "      # Evaluate model (with test logits, for dropout to be disabled)\n",
        "      correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "      accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "    # Initialize the variables (i.e. assign their default value)\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dense/kernel:0\n",
            "dense/bias:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9y9WD82ezgvY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "af347bfd-2b99-4f36-c393-f26983c3cc65"
      },
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "best_val_acc = 0.8\n",
        "log_dir = \"logs/cudnngru/%s\" % datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
        "tb_writer = tf.summary.FileWriter(log_dir, graph)\n",
        "with tf.Session(graph=graph, config=config) as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    for step in range(1, training_steps+1):\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc, summary = sess.run(\n",
        "                [loss_op, accuracy, merged_summary_op], \n",
        "                feed_dict={X: batch_x, Y: batch_y})\n",
        "            tb_writer.add_summary(summary, global_step=step)\n",
        "            tb_writer.flush()\n",
        "            # Calculate accuracy for 128 mnist test images\n",
        "            test_len = 128\n",
        "            test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
        "            test_label = mnist.test.labels[:test_len]\n",
        "            val_acc = sess.run(accuracy, feed_dict={X: test_data, Y: test_label})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc) + \", Test Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(val_acc))\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                save_path = saver.save(sess, \"/tmp/model.ckpt\", global_step=step)\n",
        "                print(\"Model saved in path: %s\" % save_path)\n",
        "    print(\"Optimization Finished!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1, Minibatch Loss= 2.2955, Training Accuracy= 0.156, Test Accuracy= 0.117\n",
            "Step 250, Minibatch Loss= 2.2108, Training Accuracy= 0.125, Test Accuracy= 0.117\n",
            "Step 500, Minibatch Loss= 0.8783, Training Accuracy= 0.781, Test Accuracy= 0.641\n",
            "Step 750, Minibatch Loss= 0.4712, Training Accuracy= 0.906, Test Accuracy= 0.734\n",
            "Step 1000, Minibatch Loss= 0.2107, Training Accuracy= 0.969, Test Accuracy= 0.812\n",
            "Model saved in path: /tmp/model.ckpt-1000\n",
            "Step 1250, Minibatch Loss= 0.4126, Training Accuracy= 0.875, Test Accuracy= 0.883\n",
            "Model saved in path: /tmp/model.ckpt-1250\n",
            "Step 1500, Minibatch Loss= 0.1344, Training Accuracy= 0.969, Test Accuracy= 0.859\n",
            "Step 1750, Minibatch Loss= 0.2449, Training Accuracy= 0.875, Test Accuracy= 0.898\n",
            "Model saved in path: /tmp/model.ckpt-1750\n",
            "Step 2000, Minibatch Loss= 0.1204, Training Accuracy= 0.969, Test Accuracy= 0.938\n",
            "Model saved in path: /tmp/model.ckpt-2000\n",
            "Step 2250, Minibatch Loss= 0.1673, Training Accuracy= 0.938, Test Accuracy= 0.930\n",
            "Step 2500, Minibatch Loss= 0.0659, Training Accuracy= 1.000, Test Accuracy= 0.945\n",
            "Model saved in path: /tmp/model.ckpt-2500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o7CephtZ6U3e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Permuted Pixel-by-pixel Sequential MNIST\n",
        "\n",
        "Increase the difficulty by shuffling the order of the sequence (by applying the same reindexing operation for all sequences)."
      ]
    },
    {
      "metadata": {
        "id": "hYUKsz2C6jiU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### CudnnGRU Model\n",
        "Basically the same. Just added a permutation operation in the graph definition."
      ]
    },
    {
      "metadata": {
        "id": "FL7UQ_u96x0-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9ba3e285-3c4c-4787-deab-550926b4aff7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1522383087376,
          "user_tz": -480,
          "elapsed": 78310,
          "user": {
            "displayName": "CeShine Lee",
            "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAYB4/FhqIeNDBAlM/s50-c-k-no/photo.jpg",
            "userId": "114938319508229761672"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    tf.set_random_seed(10)\n",
        "    # tf Graph input\n",
        "    X_ = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
        "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
        "    \n",
        "    # Permute the time steps\n",
        "    np.random.seed(100)\n",
        "    permute = np.random.permutation(784)\n",
        "    X = tf.gather(X_, permute, axis=1)   \n",
        "    \n",
        "    # Define weights\n",
        "    logits = RNN(X)\n",
        "    prediction = tf.nn.softmax(logits)\n",
        "\n",
        "    # Define loss and optimizer\n",
        "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "        logits=logits, labels=Y))\n",
        "    \n",
        "    with tf.variable_scope('optimizer'):\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.1)\n",
        "        gvs = optimizer.compute_gradients(loss_op)\n",
        "        capped_gvs = [\n",
        "            (tf.clip_by_norm(grad, 1.), var) if not var.name.startswith(\"dense\") else (grad, var)\n",
        "            for grad, var in gvs]\n",
        "        for _, var in gvs:\n",
        "            if var.name.startswith(\"dense\"):\n",
        "                print(var.name)\n",
        "        train_op = optimizer.apply_gradients(capped_gvs)  \n",
        "    \n",
        "    with tf.variable_scope('summarize_gradients'):\n",
        "        for grad, var in gvs:\n",
        "            norm = tf.norm(tf.clip_by_norm(grad, 10.), ord=2)\n",
        "            tf.summary.histogram(var.name.replace(\":\", \"_\") + '/gradient_l2', \n",
        "                                 tf.where(tf.is_nan(norm), tf.zeros_like(norm), norm))\n",
        "        for grad, var in capped_gvs:\n",
        "            norm = tf.norm(grad, ord=2)\n",
        "            tf.summary.histogram(var.name.replace(\":\", \"_\") + '/gradient_clipped_l2', \n",
        "                                 tf.where(tf.is_nan(norm), tf.zeros_like(norm), norm))\n",
        "\n",
        "    merged_summary_op = tf.summary.merge_all()\n",
        "    \n",
        "    with tf.variable_scope('evaluate'):\n",
        "      # Evaluate model (with test logits, for dropout to be disabled)\n",
        "      correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "      accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "    # Initialize the variables (i.e. assign their default value)\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dense/kernel:0\n",
            "dense/bias:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nOa_0AN17Kqy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "cellView": "code",
        "outputId": "9c0a3e3e-fac8-4898-8533-f5295423b8d6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1522383596526,
          "user_tz": -480,
          "elapsed": 508920,
          "user": {
            "displayName": "CeShine Lee",
            "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAYB4/FhqIeNDBAlM/s50-c-k-no/photo.jpg",
            "userId": "114938319508229761672"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "# Start training\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "best_val_acc = 0.8\n",
        "log_dir = \"logs/cudnngru/%s\" % datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
        "tb_writer = tf.summary.FileWriter(log_dir, graph)\n",
        "with tf.Session(graph=graph, config=config) as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    for step in range(1, training_steps+1):\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X_: batch_x, Y: batch_y})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc, summary = sess.run(\n",
        "                [loss_op, accuracy, merged_summary_op], \n",
        "                feed_dict={X_: batch_x, Y: batch_y})\n",
        "            tb_writer.add_summary(summary, global_step=step)\n",
        "            tb_writer.flush()\n",
        "            # Calculate accuracy for 128 mnist test images\n",
        "            test_len = 128\n",
        "            test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
        "            test_label = mnist.test.labels[:test_len]\n",
        "            val_acc = sess.run(accuracy, feed_dict={X_: test_data, Y: test_label})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc) + \", Test Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(val_acc))\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                save_path = saver.save(sess, \"/tmp/model.ckpt\", global_step=step)\n",
        "                print(\"Model saved in path: %s\" % save_path)\n",
        "    print(\"Optimization Finished!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1, Minibatch Loss= 2.2866, Training Accuracy= 0.188, Test Accuracy= 0.078\n",
            "Step 250, Minibatch Loss= 1.5686, Training Accuracy= 0.500, Test Accuracy= 0.508\n",
            "Step 500, Minibatch Loss= 0.8842, Training Accuracy= 0.781, Test Accuracy= 0.555\n",
            "Step 750, Minibatch Loss= 0.9858, Training Accuracy= 0.719, Test Accuracy= 0.648\n",
            "Step 1000, Minibatch Loss= 0.9501, Training Accuracy= 0.750, Test Accuracy= 0.648\n",
            "Step 1250, Minibatch Loss= 0.7780, Training Accuracy= 0.688, Test Accuracy= 0.688\n",
            "Step 1500, Minibatch Loss= 1.0259, Training Accuracy= 0.688, Test Accuracy= 0.680\n",
            "Step 1750, Minibatch Loss= 0.5693, Training Accuracy= 0.906, Test Accuracy= 0.781\n",
            "Step 2000, Minibatch Loss= 0.6409, Training Accuracy= 0.844, Test Accuracy= 0.805\n",
            "Model saved in path: /tmp/model.ckpt-2000\n",
            "Step 2250, Minibatch Loss= 0.6085, Training Accuracy= 0.812, Test Accuracy= 0.719\n",
            "Step 2500, Minibatch Loss= 0.5675, Training Accuracy= 0.812, Test Accuracy= 0.750\n",
            "Step 2750, Minibatch Loss= 0.4034, Training Accuracy= 0.938, Test Accuracy= 0.781\n",
            "Step 3000, Minibatch Loss= 0.5835, Training Accuracy= 0.812, Test Accuracy= 0.797\n",
            "Step 3250, Minibatch Loss= 0.7464, Training Accuracy= 0.719, Test Accuracy= 0.789\n",
            "Step 3500, Minibatch Loss= 0.5364, Training Accuracy= 0.844, Test Accuracy= 0.805\n",
            "Step 3750, Minibatch Loss= 0.3257, Training Accuracy= 0.906, Test Accuracy= 0.812\n",
            "Model saved in path: /tmp/model.ckpt-3750\n",
            "Step 4000, Minibatch Loss= 0.4941, Training Accuracy= 0.875, Test Accuracy= 0.789\n",
            "Step 4250, Minibatch Loss= 0.4540, Training Accuracy= 0.906, Test Accuracy= 0.836\n",
            "Model saved in path: /tmp/model.ckpt-4250\n",
            "Step 4500, Minibatch Loss= 0.3693, Training Accuracy= 0.938, Test Accuracy= 0.805\n",
            "Step 4750, Minibatch Loss= 0.4006, Training Accuracy= 0.875, Test Accuracy= 0.797\n",
            "Step 5000, Minibatch Loss= 0.5489, Training Accuracy= 0.844, Test Accuracy= 0.820\n",
            "Optimization Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E2EBK0v37Zl6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}